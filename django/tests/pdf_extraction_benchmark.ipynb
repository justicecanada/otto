{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acae18b9",
   "metadata": {},
   "source": [
    "# PDF Extraction Methods Benchmark\n",
    "\n",
    "This notebook benchmarks different PDF extraction methods in terms of:\n",
    "- **Speed**: Time taken to extract text/markdown from PDF\n",
    "- **Memory Usage**: Peak memory consumption during extraction\n",
    "- **Text Quality**: Amount and quality of extracted text\n",
    "\n",
    "We'll test each method 10 times and calculate average performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3724183",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import necessary libraries including time, memory profiling tools, data analysis libraries, and the PDF extraction modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b85851",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eccca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import statistics\n",
    "from pathlib import Path\n",
    "import tracemalloc\n",
    "import psutil\n",
    "from memory_profiler import memory_usage\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import warnings\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add Django project to path\n",
    "django_path = Path(\"/workspace/django\")\n",
    "if str(django_path) not in sys.path:\n",
    "    sys.path.insert(0, str(django_path))\n",
    "\n",
    "# Setup Django\n",
    "os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'otto.settings')\n",
    "import django\n",
    "django.setup()\n",
    "\n",
    "# Import PDF extraction functions\n",
    "from librarian.utils.process_engine import (\n",
    "    extract_markdown,\n",
    "    pdf_to_text_pymupdf,\n",
    "    pdf_to_markdown_pymupdf4llm,\n",
    "    pdf_to_markdown_pymupdf4llm_fast,\n",
    "    pdf_to_text_azure_read,\n",
    "    pdf_to_markdown_azure_layout,\n",
    "    pdf_to_markdown_via_html_azure_layout,\n",
    "    pdf_to_text_pdfium\n",
    ")\n",
    "\n",
    "# Import async utilities\n",
    "from asgiref.sync import sync_to_async\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d33f8",
   "metadata": {},
   "source": [
    "## 2. Setup Test Environment and Load PDF File\n",
    "\n",
    "Configure the test environment, set paths to the example.pdf file, and verify the file exists and is accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58809c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "TEST_PDF_PATH = Path(\"/workspace/django/tests/librarian/test_files/example.pdf\")\n",
    "BASE_RESULTS_DIR = Path(\"/workspace/django/tests\")\n",
    "\n",
    "# Create unique results folder with timestamp\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RESULTS_DIR = BASE_RESULTS_DIR / f\"pdf_benchmark_results_{timestamp}\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Verify test file exists\n",
    "if not TEST_PDF_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Test PDF file not found: {TEST_PDF_PATH}\")\n",
    "\n",
    "# Load PDF content\n",
    "with open(TEST_PDF_PATH, 'rb') as f:\n",
    "    PDF_CONTENT = f.read()\n",
    "\n",
    "print(f\"✅ Test PDF loaded successfully!\")\n",
    "print(f\"📄 File size: {len(PDF_CONTENT):,} bytes ({len(PDF_CONTENT)/1024/1024:.2f} MB)\")\n",
    "print(f\"📁 Test file: {TEST_PDF_PATH}\")\n",
    "print(f\"💾 Results will be saved to: {RESULTS_DIR}\")\n",
    "\n",
    "# Configuration\n",
    "NUM_ITERATIONS = 3\n",
    "CHUNK_SIZE = 768\n",
    "\n",
    "print(f\"🔄 Will run {NUM_ITERATIONS} iterations per method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e17be4",
   "metadata": {},
   "source": [
    "## 3. Define Memory Monitoring Utilities\n",
    "\n",
    "Create utility functions to monitor memory usage using psutil and memory_profiler, including peak memory tracking and garbage collection helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e9a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryMonitor:\n",
    "    \"\"\"Enhanced memory monitoring utilities\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.process = psutil.Process()\n",
    "        self.baseline_memory = self.get_current_memory()\n",
    "    \n",
    "    def get_current_memory(self) -> float:\n",
    "        \"\"\"Get current memory usage in MB\"\"\"\n",
    "        return self.process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    def get_memory_delta(self) -> float:\n",
    "        \"\"\"Get memory usage delta from baseline in MB\"\"\"\n",
    "        return self.get_current_memory() - self.baseline_memory\n",
    "    \n",
    "    def reset_baseline(self):\n",
    "        \"\"\"Reset baseline memory measurement\"\"\"\n",
    "        gc.collect()  # Force garbage collection\n",
    "        time.sleep(0.1)  # Small delay to allow cleanup\n",
    "        self.baseline_memory = self.get_current_memory()\n",
    "    \n",
    "    @staticmethod\n",
    "    def measure_peak_memory(func, *args, **kwargs) -> Tuple[Any, float]:\n",
    "        \"\"\"Measure peak memory usage during function execution\"\"\"\n",
    "        def wrapper():\n",
    "            return func(*args, **kwargs)\n",
    "        \n",
    "        # Use memory_profiler to track peak usage\n",
    "        mem_usage = memory_usage(wrapper, interval=0.01, timeout=300)\n",
    "        peak_memory = max(mem_usage) if mem_usage else 0\n",
    "        \n",
    "        # Execute function to get result\n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        return result, peak_memory\n",
    "\n",
    "def force_cleanup():\n",
    "    \"\"\"Force garbage collection and brief pause for cleanup\"\"\"\n",
    "    gc.collect()\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Initialize memory monitor\n",
    "memory_monitor = MemoryMonitor()\n",
    "\n",
    "print(\"✅ Memory monitoring utilities ready!\")\n",
    "print(f\"📊 Baseline memory: {memory_monitor.baseline_memory:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7093d16a",
   "metadata": {},
   "source": [
    "## 4. Implement PDF Extraction Method Wrappers\n",
    "\n",
    "Create wrapper functions for each PDF extraction method option, ensuring consistent interfaces and error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8613fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define only direct function methods for testing\n",
    "DIRECT_METHODS = {\n",
    "    'pymupdf_direct': {\n",
    "        'description': 'PyMuPDF direct text extraction',\n",
    "        'function': pdf_to_text_pymupdf,\n",
    "        'iterations': NUM_ITERATIONS\n",
    "    },\n",
    "    'pymupdf4llm_direct': {\n",
    "        'description': 'PyMuPDF4LLM direct markdown extraction',\n",
    "        'function': pdf_to_markdown_pymupdf4llm,\n",
    "        'iterations': NUM_ITERATIONS\n",
    "    },\n",
    "    'pymupdf4llm_fast_direct': {\n",
    "        'description': 'PyMuPDF4LLM fast direct markdown extraction',\n",
    "        'function': pdf_to_markdown_pymupdf4llm_fast,\n",
    "        'iterations': NUM_ITERATIONS\n",
    "    },\n",
    "    # 'pdfium_direct': {\n",
    "    #     'description': 'PDFium direct text extraction',\n",
    "    #     'function': pdf_to_text_pdfium,\n",
    "    #     'iterations': NUM_ITERATIONS\n",
    "    # },\n",
    "    'azure_read_direct': {\n",
    "        'description': 'Azure Document Intelligence Read API (direct call)',\n",
    "        'function': pdf_to_text_azure_read,\n",
    "        'iterations': 2,  # Reduced iterations for expensive Azure API\n",
    "        'is_azure': True\n",
    "    },\n",
    "    'azure_layout_direct': {\n",
    "        'description': 'Azure Document Intelligence Layout API (direct call)',\n",
    "        'function': pdf_to_markdown_via_html_azure_layout,\n",
    "        'iterations': 2,  # Reduced iterations for expensive Azure API\n",
    "        'is_azure': True\n",
    "    },\n",
    "}\n",
    "\n",
    "def extract_with_method(content: bytes, method_name: str) -> str:\n",
    "    \"\"\"Wrapper function to extract text using specified direct method\"\"\"\n",
    "    if method_name not in DIRECT_METHODS:\n",
    "        raise ValueError(f\"Unknown method: {method_name}\")\n",
    "    \n",
    "    method_config = DIRECT_METHODS[method_name]\n",
    "    extraction_function = method_config['function']\n",
    "    is_azure = method_config.get('is_azure', False)\n",
    "    \n",
    "    if is_azure:\n",
    "        # Handle Azure methods with simpler sync approach to avoid hanging\n",
    "        try:\n",
    "            # Test if we can call the function directly (some may not be truly async)\n",
    "            result = extraction_function(content)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if \"async\" in str(e).lower() or \"coroutine\" in str(e).lower():\n",
    "                # If it's truly async, use proper async handling\n",
    "                import asyncio\n",
    "                import threading\n",
    "                \n",
    "                def run_in_thread():\n",
    "                    # Create a new event loop in this thread\n",
    "                    loop = asyncio.new_event_loop()\n",
    "                    asyncio.set_event_loop(loop)\n",
    "                    try:\n",
    "                        # Convert sync function to async\n",
    "                        async def async_wrapper():\n",
    "                            # Import sync_to_async inside the async function\n",
    "                            from asgiref.sync import sync_to_async\n",
    "                            return await sync_to_async(extraction_function)(content)\n",
    "                        \n",
    "                        result = loop.run_until_complete(async_wrapper())\n",
    "                        return result\n",
    "                    finally:\n",
    "                        loop.close()\n",
    "                \n",
    "                # Run in a separate thread to avoid event loop conflicts\n",
    "                result_container = [None]\n",
    "                error_container = [None]\n",
    "                \n",
    "                def thread_target():\n",
    "                    try:\n",
    "                        result_container[0] = run_in_thread()\n",
    "                    except Exception as e:\n",
    "                        error_container[0] = e\n",
    "                \n",
    "                thread = threading.Thread(target=thread_target)\n",
    "                thread.start()\n",
    "                thread.join(timeout=60)  # 60 second timeout\n",
    "                \n",
    "                if thread.is_alive():\n",
    "                    raise TimeoutError(\"Azure extraction timed out after 60 seconds\")\n",
    "                \n",
    "                if error_container[0]:\n",
    "                    raise error_container[0]\n",
    "                \n",
    "                return result_container[0] or \"\"\n",
    "            else:\n",
    "                # Re-raise non-async errors\n",
    "                raise e\n",
    "    else:\n",
    "        # Regular synchronous extraction - direct function call\n",
    "        return extraction_function(content)\n",
    "\n",
    "def safe_extract(content: bytes, method_name: str) -> tuple[str, str]:\n",
    "    \"\"\"Safely extract text with error handling\"\"\"\n",
    "    try:\n",
    "        text = extract_with_method(content, method_name)\n",
    "        return text, None\n",
    "    except Exception as e:\n",
    "        return \"\", str(e)\n",
    "\n",
    "# Use only direct methods\n",
    "ALL_METHODS = DIRECT_METHODS\n",
    "\n",
    "print(f\"✅ Configured {len(ALL_METHODS)} direct extraction methods:\")\n",
    "for method, config in ALL_METHODS.items():\n",
    "    iterations = config.get('iterations', NUM_ITERATIONS)\n",
    "    is_azure = config.get('is_azure', False)\n",
    "    azure_note = \" (Azure API - reduced iterations)\" if is_azure else \"\"\n",
    "    function_name = config['function'].__name__\n",
    "    print(f\"  📝 {method}: {config['description']} [{iterations} iterations]{azure_note}\")\n",
    "    print(f\"      Function: {function_name}\")\n",
    "\n",
    "# Test a simple extraction to make sure everything works\n",
    "print(\"\\n🧪 Testing a simple local extraction to verify setup...\")\n",
    "try:\n",
    "    test_result = extract_with_method(PDF_CONTENT, 'pymupdf_direct')\n",
    "    print(f\"✅ Test successful! Extracted {len(test_result):,} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Test failed: {e}\")\n",
    "    print(\"This might indicate a setup issue with the PDF extraction functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e090523",
   "metadata": {},
   "source": [
    "## 5. Create Benchmarking Function\n",
    "\n",
    "Implement a comprehensive benchmarking function that measures execution time, memory usage, and captures extracted text for each method across multiple iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b5b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_method(method_name: str, content: bytes, iterations: int = None) -> Dict[str, Any]:\n",
    "    \"\"\"Benchmark a single PDF extraction method\"\"\"\n",
    "    \n",
    "    # Use method-specific iteration count if not provided\n",
    "    if iterations is None:\n",
    "        iterations = ALL_METHODS[method_name].get('iterations', NUM_ITERATIONS)\n",
    "    \n",
    "    is_azure = ALL_METHODS[method_name].get('is_azure', False)\n",
    "    azure_note = \" (Azure API)\" if is_azure else \"\"\n",
    "    \n",
    "    print(f\"\\n🔄 Benchmarking {method_name}{azure_note}...\")\n",
    "    print(f\"   Running {iterations} iterations (reduced for Azure methods)\" if is_azure else f\"   Running {iterations} iterations\")\n",
    "    \n",
    "    times = []\n",
    "    memory_peaks = []\n",
    "    memory_deltas = []\n",
    "    text_lengths = []\n",
    "    errors = []\n",
    "    full_text = \"\"  # Store the complete text output\n",
    "    all_texts = []  # Store all text outputs for consistency analysis\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        print(f\"  Iteration {i+1}/{iterations}\", end=\"... \")\n",
    "        \n",
    "        # Reset memory baseline\n",
    "        memory_monitor.reset_baseline()\n",
    "        \n",
    "        # Measure execution time and memory\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # For Azure methods, handle them differently to avoid async issues\n",
    "            if is_azure:\n",
    "                # Direct execution without memory profiler for Azure methods\n",
    "                # (memory profiler can interfere with async operations)\n",
    "                baseline_memory = memory_monitor.get_current_memory()\n",
    "                text = extract_with_method(content, method_name)\n",
    "                peak_memory = memory_monitor.get_current_memory()\n",
    "                \n",
    "                # Estimate peak memory as current memory since we can't use memory_profiler\n",
    "                peak_memory = max(baseline_memory, peak_memory)\n",
    "            else:\n",
    "                # Regular memory profiling for non-Azure methods\n",
    "                text, peak_memory = MemoryMonitor.measure_peak_memory(\n",
    "                    extract_with_method, content, method_name\n",
    "                )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            execution_time = end_time - start_time\n",
    "            memory_delta = memory_monitor.get_memory_delta()\n",
    "            text_length = len(text)\n",
    "            \n",
    "            # Store results\n",
    "            times.append(execution_time)\n",
    "            memory_peaks.append(peak_memory)\n",
    "            memory_deltas.append(memory_delta)\n",
    "            text_lengths.append(text_length)\n",
    "            all_texts.append(text)\n",
    "            \n",
    "            # Store full text from first successful iteration\n",
    "            if i == 0 and text:\n",
    "                full_text = text\n",
    "            \n",
    "            print(f\"✅ {execution_time:.2f}s, {text_length:,} chars\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            errors.append(error_msg)\n",
    "            all_texts.append(\"\")  # Add empty text for failed iterations\n",
    "            print(f\"❌ Error: {error_msg[:50]}...\")\n",
    "        \n",
    "        # Cleanup between iterations\n",
    "        force_cleanup()\n",
    "        \n",
    "        # Extra delay for Azure methods to avoid rate limiting\n",
    "        if is_azure:\n",
    "            time.sleep(2)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    successful_runs = len(times)\n",
    "    success_rate = successful_runs / iterations * 100\n",
    "    \n",
    "    # Save full text output to file\n",
    "    if full_text:\n",
    "        text_file = RESULTS_DIR / f\"{method_name}_full_text.txt\"\n",
    "        try:\n",
    "            with open(text_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"Method: {method_name}\\n\")\n",
    "                f.write(f\"Description: {ALL_METHODS[method_name]['description']}\\n\")\n",
    "                f.write(f\"Success Rate: {success_rate:.1f}%\\n\")\n",
    "                f.write(f\"Average Time: {statistics.mean(times) if times else 0:.3f}s\\n\")\n",
    "                f.write(f\"Text Length: {len(full_text):,} characters\\n\")\n",
    "                f.write(\"=\" * 80 + \"\\n\")\n",
    "                f.write(\"EXTRACTED TEXT:\\n\")\n",
    "                f.write(\"=\" * 80 + \"\\n\")\n",
    "                f.write(full_text)\n",
    "            print(f\"   💾 Full text saved to: {text_file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️  Failed to save text file: {e}\")\n",
    "    \n",
    "    results = {\n",
    "        'method': method_name,\n",
    "        'description': ALL_METHODS[method_name]['description'],\n",
    "        'iterations': iterations,\n",
    "        'successful_runs': successful_runs,\n",
    "        'success_rate': success_rate,\n",
    "        'errors': errors,\n",
    "        'full_text': full_text,  # Store the complete text\n",
    "        'sample_text': full_text[:500] + \"...\" if len(full_text) > 500 else full_text,  # Keep sample for backward compatibility\n",
    "        'is_azure': is_azure,\n",
    "        'all_texts': all_texts  # Store all text outputs for analysis\n",
    "    }\n",
    "    \n",
    "    if successful_runs > 0:\n",
    "        results.update({\n",
    "            'avg_time': statistics.mean(times),\n",
    "            'min_time': min(times),\n",
    "            'max_time': max(times),\n",
    "            'std_time': statistics.stdev(times) if len(times) > 1 else 0,\n",
    "            'avg_peak_memory': statistics.mean(memory_peaks),\n",
    "            'max_peak_memory': max(memory_peaks),\n",
    "            'avg_memory_delta': statistics.mean(memory_deltas),\n",
    "            'max_memory_delta': max(memory_deltas),\n",
    "            'avg_text_length': statistics.mean(text_lengths),\n",
    "            'min_text_length': min(text_lengths),\n",
    "            'max_text_length': max(text_lengths),\n",
    "            'text_length_consistency': (1 - statistics.stdev(text_lengths) / statistics.mean(text_lengths)) * 100 if statistics.mean(text_lengths) > 0 else 0\n",
    "        })\n",
    "    else:\n",
    "        # Fill with zeros if all runs failed\n",
    "        results.update({\n",
    "            'avg_time': 0, 'min_time': 0, 'max_time': 0, 'std_time': 0,\n",
    "            'avg_peak_memory': 0, 'max_peak_memory': 0,\n",
    "            'avg_memory_delta': 0, 'max_memory_delta': 0,\n",
    "            'avg_text_length': 0, 'min_text_length': 0, 'max_text_length': 0,\n",
    "            'text_length_consistency': 0\n",
    "        })\n",
    "    \n",
    "    print(f\"📊 Summary: {success_rate:.0f}% success, {results['avg_time']:.2f}s avg, {results['avg_text_length']:,.0f} chars avg\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✅ Benchmarking function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10fd760",
   "metadata": {},
   "source": [
    "## 6. Run Performance Tests\n",
    "\n",
    "Execute the benchmarking function for each PDF extraction method, collecting performance metrics and extracted text samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a500aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 Starting streamlined PDF extraction benchmark...\")\n",
    "print(\"📋 Testing DIRECT function calls only (no wrapper/chunking overhead)\")\n",
    "\n",
    "# Pre-test Azure functions to avoid hanging during main benchmark\n",
    "print(\"\\n🔍 Pre-testing Azure functions to check configuration...\")\n",
    "azure_methods = {k: v for k, v in ALL_METHODS.items() if v.get('is_azure', False)}\n",
    "\n",
    "# Update ALL_METHODS to only include working Azure methods\n",
    "local_methods = {k: v for k, v in ALL_METHODS.items() if not v.get('is_azure', False)}\n",
    "\n",
    "print(f\"\\n📊 Final method configuration:\")\n",
    "print(f\"   - Local methods: {len(local_methods)}\")\n",
    "print(f\"   - Azure methods: {len(azure_methods)}\")\n",
    "print(f\"   - Total methods to test: {len(ALL_METHODS)}\")\n",
    "\n",
    "# Calculate total iterations\n",
    "total_iterations = sum(method_config.get('iterations', NUM_ITERATIONS) for method_config in ALL_METHODS.values())\n",
    "print(f\"\\n🔄 Testing {len(ALL_METHODS)} direct methods with {total_iterations} total iterations\")\n",
    "print(f\"   - Local methods: {NUM_ITERATIONS} iterations each\")\n",
    "print(f\"   - Azure methods: 1 iteration each (cost optimization)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Test each method\n",
    "for i, method_name in enumerate(ALL_METHODS.keys(), 1):\n",
    "    method_config = ALL_METHODS[method_name]\n",
    "    method_iterations = method_config.get('iterations', NUM_ITERATIONS)\n",
    "    is_azure = method_config.get('is_azure', False)\n",
    "    \n",
    "    print(f\"\\n[{i}/{len(ALL_METHODS)}] Testing: {method_name}\")\n",
    "    print(f\"Description: {method_config['description']}\")\n",
    "    print(f\"Function: {method_config['function'].__name__}\")\n",
    "    \n",
    "    if is_azure:\n",
    "        print(\"⚠️  Azure API method - using reduced iterations to minimize costs\")\n",
    "    \n",
    "    try:\n",
    "        # Run benchmark with method-specific iterations\n",
    "        results = benchmark_method(method_name, PDF_CONTENT, method_iterations)\n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Brief pause between methods (longer for Azure to avoid rate limits)\n",
    "        pause_time = 3 if is_azure else 1\n",
    "        time.sleep(pause_time)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Benchmark failed for {method_name}: {e}\")\n",
    "        # Create a failed result entry\n",
    "        failed_result = {\n",
    "            'method': method_name,\n",
    "            'description': method_config['description'],\n",
    "            'iterations': method_iterations,\n",
    "            'successful_runs': 0,\n",
    "            'success_rate': 0,\n",
    "            'errors': [str(e)],\n",
    "            'full_text': '',\n",
    "            'sample_text': '',\n",
    "            'is_azure': is_azure,\n",
    "            'all_texts': [],\n",
    "            'avg_time': 0, 'min_time': 0, 'max_time': 0, 'std_time': 0,\n",
    "            'avg_peak_memory': 0, 'max_peak_memory': 0,\n",
    "            'avg_memory_delta': 0, 'max_memory_delta': 0,\n",
    "            'avg_text_length': 0, 'min_text_length': 0, 'max_text_length': 0,\n",
    "            'text_length_consistency': 0\n",
    "        }\n",
    "        all_results.append(failed_result)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✅ All direct function benchmarks completed!\")\n",
    "print(f\"📊 Collected results for {len(all_results)} methods\")\n",
    "\n",
    "# Show summary of what was tested\n",
    "local_methods = [r for r in all_results if not r.get('is_azure', False)]\n",
    "azure_methods = [r for r in all_results if r.get('is_azure', False)]\n",
    "\n",
    "print(f\"   - Local methods tested: {len(local_methods)} (with {NUM_ITERATIONS} iterations each)\")\n",
    "print(f\"   - Azure methods tested: {len(azure_methods)} (with 1 iteration each)\")\n",
    "\n",
    "if azure_methods:\n",
    "    successful_azure = [r for r in azure_methods if r['success_rate'] == 100]\n",
    "    print(f\"   - Azure methods successful: {len(successful_azure)}/{len(azure_methods)}\")\n",
    "\n",
    "print(\"\\n🎯 TESTING APPROACH:\")\n",
    "print(\"   - Direct function calls only (no extract_markdown wrapper)\")\n",
    "print(\"   - No chunking/splitting overhead included\")\n",
    "print(\"   - Pure extraction performance measurement\")\n",
    "print(\"   - Azure functions pre-tested to avoid hanging\")\n",
    "print(\"   - Timeout protection for Azure API calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a63df2",
   "metadata": {},
   "source": [
    "## 7. Analyze and Compare Results\n",
    "\n",
    "Calculate average performance metrics, compare text extraction quality, and identify the best-performing methods for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ba763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for analysis\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display summary table\n",
    "print(\"📊 PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Select key columns for summary\n",
    "summary_cols = [\n",
    "    'method', 'iterations', 'success_rate', 'avg_time', 'avg_peak_memory', \n",
    "    'avg_memory_delta', 'avg_text_length', 'text_length_consistency'\n",
    "]\n",
    "\n",
    "summary_df = df[summary_cols].copy()\n",
    "summary_df = summary_df.round({\n",
    "    'success_rate': 1,\n",
    "    'avg_time': 3,\n",
    "    'avg_peak_memory': 1,\n",
    "    'avg_memory_delta': 1,\n",
    "    'avg_text_length': 0,\n",
    "    'text_length_consistency': 1\n",
    "})\n",
    "\n",
    "# Sort by success rate and then by speed\n",
    "summary_df = summary_df.sort_values(['success_rate', 'avg_time'], ascending=[False, True])\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n🏆 TOP PERFORMERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Filter successful methods only\n",
    "successful_df = df[df['success_rate'] == 100].copy()\n",
    "\n",
    "if len(successful_df) > 0:\n",
    "    # Fastest method\n",
    "    fastest = successful_df.loc[successful_df['avg_time'].idxmin()]\n",
    "    print(f\"⚡ Fastest: {fastest['method']} ({fastest['avg_time']:.3f}s avg)\")\n",
    "    \n",
    "    # Most memory efficient (lowest peak memory)\n",
    "    memory_efficient = successful_df.loc[successful_df['avg_peak_memory'].idxmin()]\n",
    "    print(f\"💾 Most Memory Efficient: {memory_efficient['method']} ({memory_efficient['avg_peak_memory']:.1f} MB peak)\")\n",
    "    \n",
    "    # Most text extracted\n",
    "    most_text = successful_df.loc[successful_df['avg_text_length'].idxmax()]\n",
    "    print(f\"📄 Most Text Extracted: {most_text['method']} ({most_text['avg_text_length']:,.0f} chars avg)\")\n",
    "    \n",
    "    # Most consistent text length\n",
    "    if len(successful_df[successful_df['text_length_consistency'] > 0]) > 0:\n",
    "        most_consistent = successful_df.loc[successful_df['text_length_consistency'].idxmax()]\n",
    "        print(f\"🎯 Most Consistent: {most_consistent['method']} ({most_consistent['text_length_consistency']:.1f}% consistency)\")\n",
    "else:\n",
    "    print(\"❌ No methods completed successfully!\")\n",
    "\n",
    "print(\"\\n❌ FAILED METHODS\")\n",
    "print(\"=\" * 30)\n",
    "failed_df = df[df['success_rate'] < 100]\n",
    "if len(failed_df) > 0:\n",
    "    for _, row in failed_df.iterrows():\n",
    "        azure_note = \" (Azure API)\" if row.get('is_azure', False) else \"\"\n",
    "        print(f\"⚠️  {row['method']}{azure_note}: {row['success_rate']:.0f}% success rate\")\n",
    "        if row['errors']:\n",
    "            print(f\"   Error sample: {row['errors'][0][:100]}...\")\n",
    "        if row.get('is_azure', False):\n",
    "            print(\"   💡 Note: Azure methods may fail due to missing API keys or network issues\")\n",
    "else:\n",
    "    print(\"✅ All methods completed successfully!\")\n",
    "\n",
    "# Show iteration context\n",
    "print(\"\\n📋 TESTING CONTEXT\")\n",
    "print(\"=\" * 30)\n",
    "local_methods = df[~df.get('is_azure', False)]\n",
    "azure_methods = df[df.get('is_azure', False)]\n",
    "\n",
    "if len(local_methods) > 0:\n",
    "    print(f\"Local methods: {len(local_methods)} tested with {NUM_ITERATIONS} iterations each\")\n",
    "if len(azure_methods) > 0:\n",
    "    print(f\"Azure methods: {len(azure_methods)} tested with 2 iterations each (cost optimization)\")\n",
    "    if len(azure_methods[azure_methods['success_rate'] == 100]) > 0:\n",
    "        print(\"  ✅ Successfully tested Azure methods - API keys are configured\")\n",
    "    else:\n",
    "        print(\"  ⚠️  Azure methods failed - check API configuration\")\n",
    "\n",
    "# Save detailed results to the unique folder\n",
    "results_file = RESULTS_DIR / \"pdf_extraction_benchmark_results.csv\"\n",
    "summary_file = RESULTS_DIR / \"performance_summary.txt\"\n",
    "errors_file = RESULTS_DIR / \"errors_log.txt\"\n",
    "\n",
    "# Save CSV with all data\n",
    "df.to_csv(results_file, index=False)\n",
    "print(f\"\\n💾 Detailed results saved to: {results_file}\")\n",
    "\n",
    "# Save human-readable summary\n",
    "try:\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"PDF EXTRACTION BENCHMARK - PERFORMANCE SUMMARY\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Test PDF: {TEST_PDF_PATH}\\n\")\n",
    "        f.write(f\"Results saved to: {RESULTS_DIR}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"PERFORMANCE SUMMARY TABLE:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(summary_df.to_string(index=False))\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        if len(successful_df) > 0:\n",
    "            f.write(\"TOP PERFORMERS:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"⚡ Fastest: {fastest['method']} ({fastest['avg_time']:.3f}s avg)\\n\")\n",
    "            f.write(f\"💾 Most Memory Efficient: {memory_efficient['method']} ({memory_efficient['avg_peak_memory']:.1f} MB peak)\\n\")\n",
    "            f.write(f\"📄 Most Text Extracted: {most_text['method']} ({most_text['avg_text_length']:,.0f} chars avg)\\n\")\n",
    "            if len(successful_df[successful_df['text_length_consistency'] > 0]) > 0:\n",
    "                f.write(f\"🎯 Most Consistent: {most_consistent['method']} ({most_consistent['text_length_consistency']:.1f}% consistency)\\n\")\n",
    "        \n",
    "        f.write(f\"\\nTESTING CONTEXT:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        f.write(f\"Local methods: {len(local_methods)} tested with {NUM_ITERATIONS} iterations each\\n\")\n",
    "        f.write(f\"Azure methods: {len(azure_methods)} tested with 2 iterations each\\n\")\n",
    "        \n",
    "        if len(failed_df) > 0:\n",
    "            f.write(f\"\\nFAILED METHODS:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            for _, row in failed_df.iterrows():\n",
    "                azure_note = \" (Azure API)\" if row.get('is_azure', False) else \"\"\n",
    "                f.write(f\"⚠️  {row['method']}{azure_note}: {row['success_rate']:.0f}% success rate\\n\")\n",
    "    \n",
    "    print(f\"💾 Performance summary saved to: {summary_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Failed to save summary file: {e}\")\n",
    "\n",
    "# Save errors log\n",
    "try:\n",
    "    with open(errors_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"PDF EXTRACTION BENCHMARK - ERRORS LOG\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        has_errors = False\n",
    "        for _, row in df.iterrows():\n",
    "            if row['errors']:\n",
    "                has_errors = True\n",
    "                f.write(f\"METHOD: {row['method']}\\n\")\n",
    "                f.write(f\"Success Rate: {row['success_rate']:.1f}%\\n\")\n",
    "                f.write(\"Errors encountered:\\n\")\n",
    "                for i, error in enumerate(row['errors'], 1):\n",
    "                    f.write(f\"  {i}. {error}\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        if not has_errors:\n",
    "            f.write(\"✅ No errors encountered during benchmarking!\\n\")\n",
    "    \n",
    "    print(f\"💾 Errors log saved to: {errors_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Failed to save errors file: {e}\")\n",
    "\n",
    "print(f\"\\n📁 All benchmark results saved to folder: {RESULTS_DIR}\")\n",
    "print(f\"📄 Files created:\")\n",
    "print(f\"   - {results_file.name} (CSV data)\")\n",
    "print(f\"   - {summary_file.name} (Human-readable summary)\")\n",
    "print(f\"   - {errors_file.name} (Error log)\")\n",
    "print(f\"   - Individual *_full_text.txt files for each successful method\")\n",
    "print(f\"   - full_text_comparison.txt (All texts in one file)\")\n",
    "print(f\"   - text_similarity_analysis.txt (Similarity analysis)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fed9551",
   "metadata": {},
   "source": [
    "## 8. Visualize Performance Metrics\n",
    "\n",
    "Create charts and graphs to visualize speed, memory usage, and text extraction comparison across all tested methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4903f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plotting environment\n",
    "plt.rcParams['figure.figsize'] = (15, 12)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Filter successful methods for visualization\n",
    "viz_df = df[df['success_rate'] >= 50].copy()  # Include methods with at least 50% success\n",
    "\n",
    "if len(viz_df) == 0:\n",
    "    print(\"❌ No methods with sufficient success rate for visualization\")\n",
    "else:\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('PDF Extraction Methods Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Execution Time Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    bars1 = ax1.bar(range(len(viz_df)), viz_df['avg_time'], \n",
    "                    color=sns.color_palette(\"viridis\", len(viz_df)))\n",
    "    ax1.set_title('Average Execution Time', fontweight='bold')\n",
    "    ax1.set_ylabel('Time (seconds)')\n",
    "    ax1.set_xticks(range(len(viz_df)))\n",
    "    ax1.set_xticklabels(viz_df['method'], rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars1, viz_df['avg_time']):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{value:.3f}s', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 2. Peak Memory Usage\n",
    "    ax2 = axes[0, 1]\n",
    "    bars2 = ax2.bar(range(len(viz_df)), viz_df['avg_peak_memory'],\n",
    "                    color=sns.color_palette(\"plasma\", len(viz_df)))\n",
    "    ax2.set_title('Average Peak Memory Usage', fontweight='bold')\n",
    "    ax2.set_ylabel('Memory (MB)')\n",
    "    ax2.set_xticks(range(len(viz_df)))\n",
    "    ax2.set_xticklabels(viz_df['method'], rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars2, viz_df['avg_peak_memory']):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{value:.1f}MB', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 3. Text Length Extracted\n",
    "    ax3 = axes[1, 0]\n",
    "    bars3 = ax3.bar(range(len(viz_df)), viz_df['avg_text_length'],\n",
    "                    color=sns.color_palette(\"cividis\", len(viz_df)))\n",
    "    ax3.set_title('Average Text Length Extracted', fontweight='bold')\n",
    "    ax3.set_ylabel('Characters')\n",
    "    ax3.set_xticks(range(len(viz_df)))\n",
    "    ax3.set_xticklabels(viz_df['method'], rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars3, viz_df['avg_text_length']):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100,\n",
    "                f'{value:,.0f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 4. Success Rate vs Speed Scatter Plot\n",
    "    ax4 = axes[1, 1]\n",
    "    scatter = ax4.scatter(viz_df['avg_time'], viz_df['success_rate'], \n",
    "                         s=viz_df['avg_text_length']/50, \n",
    "                         c=viz_df['avg_peak_memory'], \n",
    "                         cmap='coolwarm', alpha=0.7)\n",
    "    ax4.set_title('Success Rate vs Speed\\n(bubble size = text length, color = memory)', fontweight='bold')\n",
    "    ax4.set_xlabel('Average Time (seconds)')\n",
    "    ax4.set_ylabel('Success Rate (%)')\n",
    "    \n",
    "    # Add method labels\n",
    "    for i, row in viz_df.iterrows():\n",
    "        ax4.annotate(row['method'], (row['avg_time'], row['success_rate']),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    # Add colorbar for memory usage\n",
    "    cbar = plt.colorbar(scatter, ax=ax4)\n",
    "    cbar.set_label('Peak Memory (MB)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional detailed comparison chart\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create performance efficiency plot (text/time ratio vs memory efficiency)\n",
    "    viz_df['text_per_second'] = viz_df['avg_text_length'] / viz_df['avg_time']\n",
    "    viz_df['memory_efficiency'] = viz_df['avg_text_length'] / viz_df['avg_peak_memory']\n",
    "    \n",
    "    plt.scatter(viz_df['text_per_second'], viz_df['memory_efficiency'], \n",
    "               s=200, alpha=0.7, c=range(len(viz_df)), cmap='Set3')\n",
    "    \n",
    "    # Add method labels\n",
    "    for i, row in viz_df.iterrows():\n",
    "        plt.annotate(row['method'], \n",
    "                    (row['text_per_second'], row['memory_efficiency']),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "    \n",
    "    plt.title('Performance Efficiency Analysis\\n(Higher = Better)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Text Extraction Speed (chars/second)')\n",
    "    plt.ylabel('Memory Efficiency (chars/MB)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"📊 Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d60a00",
   "metadata": {},
   "source": [
    "## 9. Text Quality Comparison\n",
    "\n",
    "Compare the actual extracted text from different methods to assess quality and differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d34d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📄 TEXT QUALITY COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display full text from each successful method\n",
    "successful_methods = df[df['success_rate'] > 0]\n",
    "\n",
    "print(f\"Found {len(successful_methods)} successful methods. Displaying full text output for each:\\n\")\n",
    "\n",
    "for i, (_, row) in enumerate(successful_methods.iterrows(), 1):\n",
    "    method = row['method']\n",
    "    full_text = row.get('full_text', '')\n",
    "    length = row['avg_text_length']\n",
    "    is_azure = row.get('is_azure', False)\n",
    "    azure_note = \" (Azure API)\" if is_azure else \"\"\n",
    "    \n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"🔍 METHOD {i}: {method.upper()}{azure_note}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"Description: {row['description']}\")\n",
    "    print(f\"Success Rate: {row['success_rate']:.1f}%\")\n",
    "    print(f\"Average Time: {row['avg_time']:.3f} seconds\")\n",
    "    print(f\"Average Text Length: {length:,.0f} characters\")\n",
    "    print(f\"Iterations Tested: {row['iterations']}\")\n",
    "    print(f\"Text Consistency: {row['text_length_consistency']:.1f}%\")\n",
    "    print(f\"{'-'*100}\")\n",
    "    print(\"FULL EXTRACTED TEXT:\")\n",
    "    print(f\"{'-'*100}\")\n",
    "    \n",
    "    if full_text:\n",
    "        # Display the full text with line numbers for large texts\n",
    "        lines = full_text.split('\\n')\n",
    "        if len(lines) > 50:  # If text is very long, add line numbers\n",
    "            print(\"(Text is long - showing with line numbers for reference)\")\n",
    "            for line_num, line in enumerate(lines[:100], 1):  # Show first 100 lines\n",
    "                print(f\"{line_num:3d}: {line}\")\n",
    "            if len(lines) > 100:\n",
    "                print(f\"... (truncated - showing first 100 of {len(lines)} lines)\")\n",
    "                print(f\"📄 Full text saved to: {method}_full_text.txt\")\n",
    "        else:\n",
    "            print(full_text)\n",
    "    else:\n",
    "        print(\"[No text extracted]\")\n",
    "    \n",
    "    print(f\"\\n{'-'*100}\")\n",
    "    print(f\"End of {method} output\")\n",
    "    print(f\"{'-'*100}\\n\")\n",
    "\n",
    "# Save a comprehensive comparison file\n",
    "comparison_file = RESULTS_DIR / \"full_text_comparison.txt\"\n",
    "try:\n",
    "    with open(comparison_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"PDF EXTRACTION METHODS - FULL TEXT COMPARISON\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Test PDF: {TEST_PDF_PATH}\\n\")\n",
    "        f.write(f\"Total Methods Tested: {len(df)}\\n\")\n",
    "        f.write(f\"Successful Methods: {len(successful_methods)}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        for _, row in successful_methods.iterrows():\n",
    "            method = row['method']\n",
    "            full_text = row.get('full_text', '')\n",
    "            is_azure = row.get('is_azure', False)\n",
    "            azure_note = \" (Azure API)\" if is_azure else \"\"\n",
    "            \n",
    "            f.write(f\"\\n{'='*100}\\n\")\n",
    "            f.write(f\"METHOD: {method.upper()}{azure_note}\\n\")\n",
    "            f.write(f\"{'='*100}\\n\")\n",
    "            f.write(f\"Description: {row['description']}\\n\")\n",
    "            f.write(f\"Success Rate: {row['success_rate']:.1f}%\\n\")\n",
    "            f.write(f\"Average Time: {row['avg_time']:.3f} seconds\\n\")\n",
    "            f.write(f\"Average Text Length: {row['avg_text_length']:,.0f} characters\\n\")\n",
    "            f.write(f\"Iterations Tested: {row['iterations']}\\n\")\n",
    "            f.write(f\"Text Consistency: {row['text_length_consistency']:.1f}%\\n\")\n",
    "            f.write(f\"{'-'*100}\\n\")\n",
    "            f.write(\"FULL EXTRACTED TEXT:\\n\")\n",
    "            f.write(f\"{'-'*100}\\n\")\n",
    "            f.write(full_text if full_text else \"[No text extracted]\")\n",
    "            f.write(f\"\\n{'-'*100}\\n\")\n",
    "            f.write(f\"End of {method} output\\n\")\n",
    "            f.write(f\"{'-'*100}\\n\\n\")\n",
    "    \n",
    "    print(f\"💾 Comprehensive text comparison saved to: {comparison_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Failed to save comparison file: {e}\")\n",
    "\n",
    "# Text quality metrics\n",
    "print(\"\\n📈 TEXT QUALITY METRICS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "quality_df = successful_methods[['method', 'avg_text_length', 'text_length_consistency', 'avg_time']].copy()\n",
    "quality_df = quality_df.sort_values('avg_text_length', ascending=False)\n",
    "\n",
    "print(\"Ranking by Text Length:\")\n",
    "for i, (_, row) in enumerate(quality_df.iterrows(), 1):\n",
    "    is_azure = df[df['method'] == row['method']].iloc[0].get('is_azure', False)\n",
    "    azure_note = \" (Azure)\" if is_azure else \"\"\n",
    "    print(f\"  {i}. {row['method']}{azure_note}: {row['avg_text_length']:,.0f} chars, {row['avg_time']:.3f}s, {row['text_length_consistency']:.1f}% consistency\")\n",
    "\n",
    "# Text similarity analysis\n",
    "print(f\"\\n🔄 TEXT SIMILARITY ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if len(successful_methods) > 1:\n",
    "    # Simple similarity check - count overlapping words\n",
    "    print(\"Checking text overlap between methods...\")\n",
    "    \n",
    "    similarity_results = []\n",
    "    methods_with_text = [(row['method'], row.get('full_text', '')) for _, row in successful_methods.iterrows() if row.get('full_text', '')]\n",
    "    \n",
    "    for i, (method1, text1) in enumerate(methods_with_text):\n",
    "        for j, (method2, text2) in enumerate(methods_with_text[i+1:], i+1):\n",
    "            if text1 and text2:\n",
    "                # Simple word-based similarity\n",
    "                words1 = set(text1.lower().split())\n",
    "                words2 = set(text2.lower().split())\n",
    "                \n",
    "                if words1 and words2:\n",
    "                    overlap = len(words1.intersection(words2))\n",
    "                    total_unique = len(words1.union(words2))\n",
    "                    similarity = (overlap / total_unique) * 100 if total_unique > 0 else 0\n",
    "                    \n",
    "                    similarity_results.append({\n",
    "                        'method1': method1,\n",
    "                        'method2': method2,\n",
    "                        'similarity': similarity,\n",
    "                        'overlap_words': overlap,\n",
    "                        'total_unique_words': total_unique\n",
    "                    })\n",
    "    \n",
    "    if similarity_results:\n",
    "        # Sort by similarity\n",
    "        similarity_results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        \n",
    "        print(\"Text similarity between methods (word overlap %):\")\n",
    "        for result in similarity_results[:10]:  # Show top 10 most similar\n",
    "            print(f\"  {result['method1']} ↔ {result['method2']}: {result['similarity']:.1f}% similarity ({result['overlap_words']} common words)\")\n",
    "        \n",
    "        # Save similarity analysis\n",
    "        similarity_file = RESULTS_DIR / \"text_similarity_analysis.txt\"\n",
    "        try:\n",
    "            with open(similarity_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"TEXT SIMILARITY ANALYSIS\\n\")\n",
    "                f.write(\"=\" * 40 + \"\\n\")\n",
    "                f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "                \n",
    "                for result in similarity_results:\n",
    "                    f.write(f\"{result['method1']} ↔ {result['method2']}\\n\")\n",
    "                    f.write(f\"  Similarity: {result['similarity']:.1f}%\\n\")\n",
    "                    f.write(f\"  Common words: {result['overlap_words']}\\n\")\n",
    "                    f.write(f\"  Total unique words: {result['total_unique_words']}\\n\\n\")\n",
    "            \n",
    "            print(f\"💾 Similarity analysis saved to: {similarity_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Failed to save similarity analysis: {e}\")\n",
    "\n",
    "print(\"\\n✅ Complete text quality analysis finished!\")\n",
    "print(f\"📁 All text files saved to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b91316",
   "metadata": {},
   "source": [
    "## 10. Final Recommendations\n",
    "\n",
    "Provide recommendations based on the benchmark results for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c68d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 RECOMMENDATIONS BASED ON DIRECT FUNCTION BENCHMARK RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "successful_methods = df[df['success_rate'] == 100].copy()\n",
    "\n",
    "if len(successful_methods) > 0:\n",
    "    print(\"\\n🚀 FOR SPEED (fastest direct extraction):\")\n",
    "    speed_winner = successful_methods.loc[successful_methods['avg_time'].idxmin()]\n",
    "    azure_note = \" (Azure API)\" if speed_winner.get('is_azure', False) else \"\"\n",
    "    function_name = ALL_METHODS[speed_winner['method']]['function'].__name__\n",
    "    print(f\"   Recommended: {speed_winner['method']}{azure_note}\")\n",
    "    print(f\"   Function: {function_name}\")\n",
    "    print(f\"   Average time: {speed_winner['avg_time']:.3f} seconds\")\n",
    "    print(f\"   Text quality: {speed_winner['avg_text_length']:,.0f} characters\")\n",
    "    print(f\"   Tested with: {speed_winner['iterations']} iterations\")\n",
    "    \n",
    "    print(\"\\n💾 FOR MEMORY EFFICIENCY (lowest memory usage):\")\n",
    "    memory_winner = successful_methods.loc[successful_methods['avg_peak_memory'].idxmin()]\n",
    "    azure_note = \" (Azure API)\" if memory_winner.get('is_azure', False) else \"\"\n",
    "    function_name = ALL_METHODS[memory_winner['method']]['function'].__name__\n",
    "    print(f\"   Recommended: {memory_winner['method']}{azure_note}\")\n",
    "    print(f\"   Function: {function_name}\")\n",
    "    print(f\"   Peak memory: {memory_winner['avg_peak_memory']:.1f} MB\")\n",
    "    print(f\"   Execution time: {memory_winner['avg_time']:.3f} seconds\")\n",
    "    print(f\"   Tested with: {memory_winner['iterations']} iterations\")\n",
    "    \n",
    "    print(\"\\n📄 FOR TEXT QUALITY (most comprehensive extraction):\")\n",
    "    quality_winner = successful_methods.loc[successful_methods['avg_text_length'].idxmax()]\n",
    "    azure_note = \" (Azure API)\" if quality_winner.get('is_azure', False) else \"\"\n",
    "    function_name = ALL_METHODS[quality_winner['method']]['function'].__name__\n",
    "    print(f\"   Recommended: {quality_winner['method']}{azure_note}\")\n",
    "    print(f\"   Function: {function_name}\")\n",
    "    print(f\"   Text extracted: {quality_winner['avg_text_length']:,.0f} characters\")\n",
    "    print(f\"   Execution time: {quality_winner['avg_time']:.3f} seconds\")\n",
    "    print(f\"   Tested with: {quality_winner['iterations']} iterations\")\n",
    "    \n",
    "    print(\"\\n⚖️ FOR BALANCED PERFORMANCE (speed + quality + memory):\")\n",
    "    # Calculate composite score (normalize metrics and combine)\n",
    "    successful_methods = successful_methods.copy()\n",
    "    \n",
    "    # Normalize metrics (0-1 scale, higher is better)\n",
    "    if len(successful_methods) > 1:\n",
    "        successful_methods['speed_score'] = (successful_methods['avg_time'].max() - successful_methods['avg_time']) / (successful_methods['avg_time'].max() - successful_methods['avg_time'].min())\n",
    "        successful_methods['memory_score'] = (successful_methods['avg_peak_memory'].max() - successful_methods['avg_peak_memory']) / (successful_methods['avg_peak_memory'].max() - successful_methods['avg_peak_memory'].min())\n",
    "        successful_methods['quality_score'] = (successful_methods['avg_text_length'] - successful_methods['avg_text_length'].min()) / (successful_methods['avg_text_length'].max() - successful_methods['avg_text_length'].min())\n",
    "    else:\n",
    "        successful_methods['speed_score'] = 1\n",
    "        successful_methods['memory_score'] = 1\n",
    "        successful_methods['quality_score'] = 1\n",
    "    \n",
    "    # Composite score (equal weights)\n",
    "    successful_methods['composite_score'] = (successful_methods['speed_score'] + successful_methods['memory_score'] + successful_methods['quality_score']) / 3\n",
    "    \n",
    "    balanced_winner = successful_methods.loc[successful_methods['composite_score'].idxmax()]\n",
    "    azure_note = \" (Azure API)\" if balanced_winner.get('is_azure', False) else \"\"\n",
    "    function_name = ALL_METHODS[balanced_winner['method']]['function'].__name__\n",
    "    print(f\"   Recommended: {balanced_winner['method']}{azure_note}\")\n",
    "    print(f\"   Function: {function_name}\")\n",
    "    print(f\"   Composite score: {balanced_winner['composite_score']:.3f}\")\n",
    "    print(f\"   Time: {balanced_winner['avg_time']:.3f}s, Memory: {balanced_winner['avg_peak_memory']:.1f}MB, Text: {balanced_winner['avg_text_length']:,.0f} chars\")\n",
    "    print(f\"   Tested with: {balanced_winner['iterations']} iterations\")\n",
    "    \n",
    "    # Separate recommendations for local vs Azure methods\n",
    "    local_successful = successful_methods[~successful_methods.get('is_azure', False)]\n",
    "    azure_successful = successful_methods[successful_methods.get('is_azure', False)]\n",
    "    \n",
    "    if len(local_successful) > 0:\n",
    "        print(\"\\n🏠 BEST LOCAL FUNCTION (no API costs):\")\n",
    "        local_winner = local_successful.loc[local_successful['composite_score'].idxmax()]\n",
    "        function_name = ALL_METHODS[local_winner['method']]['function'].__name__\n",
    "        print(f\"   Recommended: {local_winner['method']}\")\n",
    "        print(f\"   Function: {function_name}\")\n",
    "        print(f\"   Time: {local_winner['avg_time']:.3f}s, Memory: {local_winner['avg_peak_memory']:.1f}MB, Text: {local_winner['avg_text_length']:,.0f} chars\")\n",
    "        print(f\"   Advantage: No API costs, fast processing, good for high-volume use\")\n",
    "    \n",
    "    if len(azure_successful) > 0:\n",
    "        print(\"\\n☁️  BEST AZURE FUNCTION (API-based, may have costs):\")\n",
    "        azure_winner = azure_successful.loc[azure_successful['composite_score'].idxmax()]\n",
    "        function_name = ALL_METHODS[azure_winner['method']]['function'].__name__\n",
    "        print(f\"   Recommended: {azure_winner['method']}\")\n",
    "        print(f\"   Function: {function_name}\")\n",
    "        print(f\"   Time: {azure_winner['avg_time']:.3f}s, Memory: {azure_winner['avg_peak_memory']:.1f}MB, Text: {azure_winner['avg_text_length']:,.0f} chars\")\n",
    "        print(f\"   Advantage: Professional OCR, handles complex layouts, good for accuracy-critical use\")\n",
    "        print(f\"   Note: Requires Azure API keys and incurs usage costs\")\n",
    "    \n",
    "    print(\"\\n⚠️  METHODS TO AVOID (if any failed):\")\n",
    "    failed_methods = df[df['success_rate'] < 100]\n",
    "    if len(failed_methods) > 0:\n",
    "        for _, row in failed_methods.iterrows():\n",
    "            azure_note = \" (Azure API)\" if row.get('is_azure', False) else \"\"\n",
    "            function_name = ALL_METHODS[row['method']]['function'].__name__\n",
    "            print(f\"   ❌ {row['method']}{azure_note}: {row['success_rate']:.0f}% success rate\")\n",
    "            print(f\"      Function: {function_name}\")\n",
    "            if row.get('is_azure', False):\n",
    "                print(f\"      💡 Azure method failure may be due to missing API keys or network issues\")\n",
    "    else:\n",
    "        print(\"   ✅ All direct functions performed reliably!\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No successful methods found - all direct functions failed!\")\n",
    "    print(\"This might indicate:\")\n",
    "    print(\"  - Missing dependencies (Azure API keys, etc.)\")\n",
    "    print(\"  - Corrupted test PDF file\")\n",
    "    print(\"  - Environment configuration issues\")\n",
    "\n",
    "# Save recommendations to file\n",
    "recommendations_file = RESULTS_DIR / \"recommendations.txt\"\n",
    "try:\n",
    "    with open(recommendations_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"PDF EXTRACTION BENCHMARK - DIRECT FUNCTIONS RECOMMENDATIONS\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Based on analysis of {len(df)} direct extraction functions\\n\")\n",
    "        f.write(\"Testing approach: Direct function calls only (no wrapper overhead)\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "        \n",
    "        if len(successful_methods) > 0:\n",
    "            f.write(\"🚀 FOR SPEED (fastest processing):\\n\")\n",
    "            function_name = ALL_METHODS[speed_winner['method']]['function'].__name__\n",
    "            f.write(f\"   Recommended: {speed_winner['method']}\\n\")\n",
    "            f.write(f\"   Function: {function_name}\\n\")\n",
    "            f.write(f\"   Average time: {speed_winner['avg_time']:.3f} seconds\\n\")\n",
    "            f.write(f\"   Text quality: {speed_winner['avg_text_length']:,.0f} characters\\n\\n\")\n",
    "            \n",
    "            f.write(\"💾 FOR MEMORY EFFICIENCY (lowest memory usage):\\n\")\n",
    "            function_name = ALL_METHODS[memory_winner['method']]['function'].__name__\n",
    "            f.write(f\"   Recommended: {memory_winner['method']}\\n\")\n",
    "            f.write(f\"   Function: {function_name}\\n\")\n",
    "            f.write(f\"   Peak memory: {memory_winner['avg_peak_memory']:.1f} MB\\n\")\n",
    "            f.write(f\"   Execution time: {memory_winner['avg_time']:.3f} seconds\\n\\n\")\n",
    "            \n",
    "            f.write(\"📄 FOR TEXT QUALITY (most comprehensive extraction):\\n\")\n",
    "            function_name = ALL_METHODS[quality_winner['method']]['function'].__name__\n",
    "            f.write(f\"   Recommended: {quality_winner['method']}\\n\")\n",
    "            f.write(f\"   Function: {function_name}\\n\")\n",
    "            f.write(f\"   Text extracted: {quality_winner['avg_text_length']:,.0f} characters\\n\")\n",
    "            f.write(f\"   Execution time: {quality_winner['avg_time']:.3f} seconds\\n\\n\")\n",
    "            \n",
    "            f.write(\"⚖️ FOR BALANCED PERFORMANCE:\\n\")\n",
    "            function_name = ALL_METHODS[balanced_winner['method']]['function'].__name__\n",
    "            f.write(f\"   Recommended: {balanced_winner['method']}\\n\")\n",
    "            f.write(f\"   Function: {function_name}\\n\")\n",
    "            f.write(f\"   Composite score: {balanced_winner['composite_score']:.3f}\\n\")\n",
    "            f.write(f\"   Time: {balanced_winner['avg_time']:.3f}s, Memory: {balanced_winner['avg_peak_memory']:.1f}MB, Text: {balanced_winner['avg_text_length']:,.0f} chars\\n\\n\")\n",
    "            \n",
    "            if len(local_successful) > 0:\n",
    "                f.write(\"🏠 BEST LOCAL FUNCTION (no API costs):\\n\")\n",
    "                function_name = ALL_METHODS[local_winner['method']]['function'].__name__\n",
    "                f.write(f\"   Recommended: {local_winner['method']}\\n\")\n",
    "                f.write(f\"   Function: {function_name}\\n\")\n",
    "                f.write(f\"   Advantage: No API costs, fast processing, good for high-volume use\\n\\n\")\n",
    "            \n",
    "            if len(azure_successful) > 0:\n",
    "                f.write(\"☁️ BEST AZURE FUNCTION (API-based):\\n\")\n",
    "                function_name = ALL_METHODS[azure_winner['method']]['function'].__name__\n",
    "                f.write(f\"   Recommended: {azure_winner['method']}\\n\")\n",
    "                f.write(f\"   Function: {function_name}\\n\")\n",
    "                f.write(f\"   Advantage: Professional OCR, handles complex layouts\\n\")\n",
    "                f.write(f\"   Note: Requires Azure API keys and incurs usage costs\\n\\n\")\n",
    "        \n",
    "        f.write(\"DIRECT FUNCTION MAPPING:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        for method, config in ALL_METHODS.items():\n",
    "            f.write(f\"• {method}: {config['function'].__name__}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"USE CASE RECOMMENDATIONS:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        f.write(\"• Speed: For high-volume processing\\n\")\n",
    "        f.write(\"• Memory: For resource-constrained environments\\n\")\n",
    "        f.write(\"• Quality: For accuracy-critical applications\\n\")\n",
    "        f.write(\"• Cost: Local functions are free, Azure functions have API costs\\n\")\n",
    "        f.write(\"• Balance: For general-purpose usage\\n\")\n",
    "    \n",
    "    print(f\"\\n💾 Recommendations saved to: {recommendations_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Failed to save recommendations file: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🏁 DIRECT FUNCTION BENCHMARK COMPLETE!\")\n",
    "print(f\"📊 Tested {len(ALL_METHODS)} direct extraction functions:\")\n",
    "print(f\"   - Local functions: {len(df[~df.get('is_azure', False)])} tested with {NUM_ITERATIONS} iterations each\")\n",
    "print(f\"   - Azure functions: {len(df[df.get('is_azure', False)])} tested with 1 iteration each\")\n",
    "\n",
    "print(f\"\\n📁 ALL RESULTS SAVED TO: {RESULTS_DIR}\")\n",
    "print(\"📄 Files created:\")\n",
    "results_files = [\n",
    "    \"pdf_extraction_benchmark_results.csv (Complete data)\",\n",
    "    \"performance_summary.txt (Human-readable summary)\",\n",
    "    \"recommendations.txt (Function recommendations)\",\n",
    "    \"errors_log.txt (Error details)\",\n",
    "    \"full_text_comparison.txt (All extracted texts)\",\n",
    "    \"text_similarity_analysis.txt (Text similarity)\",\n",
    "    \"*_full_text.txt (Individual function outputs)\"\n",
    "]\n",
    "\n",
    "for file_desc in results_files:\n",
    "    print(f\"   • {file_desc}\")\n",
    "\n",
    "print(\"\\n🎯 TESTING APPROACH:\")\n",
    "print(\"  - Direct function calls only (no extract_markdown wrapper)\")\n",
    "print(\"  - No chunking/splitting overhead included in measurements\")\n",
    "print(\"  - Pure extraction performance of individual functions\")\n",
    "print(\"  - Async handling maintained for Azure functions\")\n",
    "\n",
    "print(\"\\n💡 ITERATION COUNT NOTES:\")\n",
    "print(\"  - Local functions: Full 3 iterations for statistical reliability\")\n",
    "print(\"  - Azure functions: Only 1 iteration to minimize API costs\")\n",
    "print(\"  - Consider this when comparing performance between local and Azure functions\")\n",
    "\n",
    "print(\"\\n🔍 TEXT ANALYSIS:\")\n",
    "print(\"  - Full text output displayed above and saved to individual files\")\n",
    "print(\"  - Text similarity analysis performed between functions\")\n",
    "print(\"  - Complete comparison available in full_text_comparison.txt\")\n",
    "\n",
    "print(\"\\nConsider the specific needs of your use case when choosing a function:\")\n",
    "print(\"  - Speed: For high-volume processing\")\n",
    "print(\"  - Memory: For resource-constrained environments\")\n",
    "print(\"  - Quality: For accuracy-critical applications\")\n",
    "print(\"  - Cost: Local functions are free, Azure functions have API costs\")\n",
    "print(\"  - Balance: For general-purpose usage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
